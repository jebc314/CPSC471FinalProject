{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from EnsembleXAI.Ensemble import normEnsembleXAI\n",
    "from EnsembleXAI.Normalization import mean_var_normalize\n",
    "from EnsembleXAI.Metrics import accordance_precision, accordance_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom F1_score so that when precision and recall = 0 (F1 nan) that is just interpretted as zero \n",
    "# (throwing away the consideration of that point or making really bad -> just bad)\n",
    "def F1_score(\n",
    "        explanations: torch.Tensor, masks: torch.Tensor, threshold: float = 0.0\n",
    ") -> float:\n",
    "    acc_recall = accordance_recall(explanations, masks, threshold=threshold)\n",
    "    acc_prec = accordance_precision(explanations, masks, threshold=threshold)\n",
    "    values = 2 * (acc_recall * acc_prec) / (acc_recall + acc_prec)\n",
    "    values[values != values] = 0\n",
    "    value = torch.sum(values) / values.shape[0]\n",
    "    return value.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions = {\n",
    "    'attributions_ig': torch.load('ImageNet/attributions_ig.pt'),\n",
    "    'attributions_s': torch.load('ImageNet/attributions_s.pt'),\n",
    "    'attributions_gs': torch.load('ImageNet/attributions_gs.pt'),\n",
    "    'attributions_gb': torch.load('ImageNet/attributions_gb.pt'),\n",
    "    'attributions_d': torch.load('ImageNet/attributions_d.pt'),\n",
    "    'attributions_ixg': torch.load('ImageNet/attributions_ixg.pt'),\n",
    "    'attributions_l': torch.load('ImageNet/attributions_l.pt'),\n",
    "    'attributions_o': torch.load('ImageNet/attributions_o.pt'),\n",
    "    'attributions_svs': torch.load('ImageNet/attributions_svs.pt'),\n",
    "    'attributions_fa': torch.load('ImageNet/attributions_fa.pt'),\n",
    "    'attributions_ks': torch.load('ImageNet/attributions_ks.pt'),\n",
    "    'attributions_nt': torch.load('ImageNet/attributions_nt.pt'),\n",
    "}\n",
    "\n",
    "normalized_attributions = {attr: mean_var_normalize(attributions[attr]) for attr in attributions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_masks = torch.load(\"ImageNet/ground_truth_masks.pt\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions_weight = {\n",
    "    attribution: F1_score(attributions[attribution], ground_truth_masks) for attribution in attributions\n",
    "}\n",
    "weight_sum = sum([attributions_weight[attribution] for attribution in attributions_weight])\n",
    "attributions_weight = {\n",
    "    attribution: attributions_weight[attribution] / weight_sum for attribution in attributions\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attributions_ig': 0.07576233122529771,\n",
       " 'attributions_s': 0.09991986593436414,\n",
       " 'attributions_gs': 0.07570613808468314,\n",
       " 'attributions_gb': 0.07502384027356462,\n",
       " 'attributions_d': 0.0710520701080619,\n",
       " 'attributions_ixg': 0.07553182139136243,\n",
       " 'attributions_l': 0.08509368208397593,\n",
       " 'attributions_o': 0.08121739650805401,\n",
       " 'attributions_svs': 0.1013098916247693,\n",
       " 'attributions_fa': 0.0910585615316911,\n",
       " 'attributions_ks': 0.09256187064423206,\n",
       " 'attributions_nt': 0.07576253058994363}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributions_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions_weighted = {\n",
    "    attribution: attributions_weight[attribution] * attributions[attribution] for attribution in attributions\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08333333333333333\n"
     ]
    }
   ],
   "source": [
    "threshold = 1 / len(attributions)\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1 / len(attributions)\n",
    "attributions_trimmed = {}\n",
    "for attribution in attributions_weight:\n",
    "    if attributions_weight[attribution] < threshold:\n",
    "        continue\n",
    "    attributions_trimmed[attribution] = attributions[attribution]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['attributions_s', 'attributions_l', 'attributions_svs', 'attributions_fa', 'attributions_ks'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributions_trimmed.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_weighted_attributions = {attr: mean_var_normalize(attributions_weighted[attr]) for attr in attributions_weighted}\n",
    "normalized_trimmed_attributions = {attr: mean_var_normalize(attributions_trimmed[attr]) for attr in attributions_trimmed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "_attributions = {attr: mean_var_normalize(normalized_weighted_attributions[attr]) for attr in attributions}\n",
    "explanations = torch.stack([torch.abs(normalized_weighted_attributions[attr]) for attr in normalized_weighted_attributions], dim=1)\n",
    "agg = normEnsembleXAI(explanations.detach(), aggregating_func='avg')\n",
    "torch.save(agg, \"ImageNet/weighted_agg.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "_attributions = {attr: mean_var_normalize(normalized_trimmed_attributions[attr]) for attr in normalized_trimmed_attributions}\n",
    "explanations = torch.stack([torch.abs(normalized_trimmed_attributions[attr]) for attr in normalized_trimmed_attributions], dim=1)\n",
    "agg = normEnsembleXAI(explanations.detach(), aggregating_func='avg')\n",
    "torch.save(agg, \"ImageNet/trimmed_agg.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs471_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
